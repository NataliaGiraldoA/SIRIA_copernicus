{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T19:21:16.455539Z",
     "start_time": "2025-08-02T19:21:16.438535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter"
   ],
   "id": "59e1c21b51309651",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T20:38:22.351302Z",
     "start_time": "2025-08-02T20:22:52.931198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_size = (384, 384)\n",
    "batch_size = 16\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "    'C:/Users/Natalia/Desktop/PADIA/SIRIA_copernicus/dataset_soil/training',\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    'C:/Users/Natalia/Desktop/PADIA/SIRIA_copernicus/dataset_soil/validation',\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_ds = image_dataset_from_directory(\n",
    "    'C:/Users/Natalia/Desktop/PADIA/SIRIA_copernicus/dataset_soil/testing',\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Calcular class weights y class distribution\n",
    "def get_class_distribution(dataset):\n",
    "    class_counts = {}\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        for label in labels.numpy():\n",
    "            class_name = dataset.class_names[label]\n",
    "            class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
    "            total_samples += 1\n",
    "\n",
    "    print(\"Distribución de clases:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        percentage = (count / total_samples) * 100\n",
    "        print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "    return class_counts, total_samples\n",
    "\n",
    "class_counts, total_samples = get_class_distribution(train_ds)\n",
    "\n",
    "def calculate_moderate_class_weights(class_counts, class_names): #Calcular el class weights moderado para balancear\n",
    "\n",
    "    y_integers = []\n",
    "    for images, labels in train_ds:\n",
    "        y_integers.extend(labels.numpy())\n",
    "\n",
    "\n",
    "    full_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_integers),\n",
    "        y=y_integers\n",
    "    )\n",
    "\n",
    "    moderate_weights = np.sqrt(full_weights)\n",
    "\n",
    "    min_weight = np.min(moderate_weights) #Normalizar\n",
    "    moderate_weights = moderate_weights / min_weight\n",
    "\n",
    "    class_weight_dict = dict(zip(np.unique(y_integers), moderate_weights))\n",
    "\n",
    "    print(\"\\nClass weights moderados calculados:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"  {class_name}: {class_weight_dict[i]:.3f}\")\n",
    "\n",
    "    return class_weight_dict\n",
    "\n",
    "class_weights = calculate_moderate_class_weights(class_counts, train_ds.class_names)\n",
    "\n",
    "@register_keras_serializable()\n",
    "class RandomSaturation(layers.Layer):\n",
    "    def __init__(self, factor_min=0.8, factor_max=1.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.factor_min = factor_min\n",
    "        self.factor_max = factor_max\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            factor = tf.random.uniform([], self.factor_min, self.factor_max)\n",
    "            return tf.image.adjust_saturation(inputs, factor)\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'factor_min': self.factor_min,\n",
    "            'factor_max': self.factor_max\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class RandomHue(layers.Layer):\n",
    "    def __init__(self, delta_min=-0.05, delta_max=0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.delta_min = delta_min\n",
    "        self.delta_max = delta_max\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            delta = tf.random.uniform([], self.delta_min, self.delta_max)\n",
    "            return tf.image.adjust_hue(inputs, delta)\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'delta_min': self.delta_min,\n",
    "            'delta_max': self.delta_max\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.3),  # Aumentado de 0.2 a 0.3\n",
    "    layers.RandomZoom(0.2),      # Aumentado de 0.15 a 0.2\n",
    "    layers.RandomContrast(0.2),  # Aumentado de 0.15 a 0.2\n",
    "    layers.RandomBrightness(0.15), # Aumentado de 0.1 a 0.15\n",
    "    layers.RandomTranslation(0.15, 0.15), # Aumentado de 0.1 a 0.15\n",
    "    RandomSaturation(0.7, 1.3),  # Más agresivo: de (0.8,1.2) a (0.7,1.3)\n",
    "    RandomHue(-0.08, 0.08),      # Más agresivo: de (-0.05,0.05) a (-0.08,0.08)\n",
    "    # NUEVO: Añadir ruido gaussiano ocasional\n",
    "    layers.GaussianNoise(0.01),\n",
    "])\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.keras.applications.efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "train_ds = train_ds.map(preprocess)\n",
    "val_ds = val_ds.map(preprocess)\n",
    "test_ds = test_ds.map(preprocess)\n",
    "\n",
    "\n",
    "from keras.src.applications.efficientnet import EfficientNetB0\n",
    "base_model = EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(384, 384, 3)\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Resizing(384, 384),\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.6),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    # MEJORA 4: Capa adicional más pequeña\n",
    "    layers.Dense(64, activation='relu',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005, weight_decay=0.0001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    "    min_delta=0.001\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,\n",
    "    patience=5,\n",
    "    min_lr=1e-9,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\ntraining con class weights\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "def calculate_detailed_metrics(model, dataset, dataset_name, class_names):\n",
    "    print(f\"\\n{dataset_name}\")\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        predictions = model.predict(images, verbose=0)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(predicted_classes)\n",
    "        y_prob.extend(predictions)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy:.2%})\")\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    report = classification_report(y_true, y_pred,\n",
    "                                 target_names=class_names,\n",
    "                                 digits=4)\n",
    "    print(report)\n",
    "\n",
    "    print(f\"\\nConfianza promedi:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = y_true == i\n",
    "        if np.sum(class_mask) > 0:\n",
    "            avg_confidence = np.mean(np.max(y_prob[class_mask], axis=1))\n",
    "            print(f\"  {class_name}: {avg_confidence:.4f}\")\n",
    "\n",
    "print(\"despues de entrenamiento balanceado\")\n",
    "val_loss, val_acc = model.evaluate(val_ds)\n",
    "print(f\"Precisión en validación: {val_acc:.2%}\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f\"Precisión en test: {test_acc:.2%}\")\n",
    "\n",
    "calculate_detailed_metrics(model, val_ds, \"Validación\", class_names)\n",
    "calculate_detailed_metrics(model, test_ds, \"Test\", class_names)\n"
   ],
   "id": "29ac4e96c84ef972",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1131 files belonging to 4 classes.\n",
      "Found 169 files belonging to 4 classes.\n",
      "Found 169 files belonging to 4 classes.\n",
      "Distribución de clases:\n",
      "  Alluvial soil: 495 samples (43.8%)\n",
      "  Black Soil: 228 samples (20.2%)\n",
      "  Clay soil: 169 samples (14.9%)\n",
      "  Red soil: 239 samples (21.1%)\n",
      "\n",
      "Class weights moderados calculados:\n",
      "  Alluvial soil: 1.000\n",
      "  Black Soil: 1.473\n",
      "  Clay soil: 1.711\n",
      "  Red soil: 1.439\n",
      "\n",
      "training con class weights\n",
      "Epoch 1/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 667ms/step - accuracy: 0.3239 - loss: 8.8269 - val_accuracy: 0.7101 - val_loss: 7.4976 - learning_rate: 5.0000e-04\n",
      "Epoch 2/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 643ms/step - accuracy: 0.4433 - loss: 8.0053 - val_accuracy: 0.7751 - val_loss: 6.8889 - learning_rate: 5.0000e-04\n",
      "Epoch 3/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 665ms/step - accuracy: 0.5509 - loss: 7.3661 - val_accuracy: 0.8402 - val_loss: 6.3306 - learning_rate: 5.0000e-04\n",
      "Epoch 4/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 633ms/step - accuracy: 0.6092 - loss: 6.8041 - val_accuracy: 0.8402 - val_loss: 5.8743 - learning_rate: 5.0000e-04\n",
      "Epoch 5/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 636ms/step - accuracy: 0.6771 - loss: 6.3465 - val_accuracy: 0.8757 - val_loss: 5.4545 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 621ms/step - accuracy: 0.6801 - loss: 5.9264 - val_accuracy: 0.8698 - val_loss: 5.0987 - learning_rate: 5.0000e-04\n",
      "Epoch 7/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 621ms/step - accuracy: 0.7330 - loss: 5.5202 - val_accuracy: 0.8817 - val_loss: 4.7539 - learning_rate: 5.0000e-04\n",
      "Epoch 8/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 624ms/step - accuracy: 0.7589 - loss: 5.2191 - val_accuracy: 0.8757 - val_loss: 4.4833 - learning_rate: 5.0000e-04\n",
      "Epoch 9/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 623ms/step - accuracy: 0.7667 - loss: 4.8878 - val_accuracy: 0.8817 - val_loss: 4.2101 - learning_rate: 5.0000e-04\n",
      "Epoch 10/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 630ms/step - accuracy: 0.7735 - loss: 4.5961 - val_accuracy: 0.8994 - val_loss: 3.9587 - learning_rate: 5.0000e-04\n",
      "Epoch 11/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 631ms/step - accuracy: 0.7936 - loss: 4.3361 - val_accuracy: 0.8994 - val_loss: 3.7452 - learning_rate: 5.0000e-04\n",
      "Epoch 12/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 626ms/step - accuracy: 0.7730 - loss: 4.1732 - val_accuracy: 0.8876 - val_loss: 3.5179 - learning_rate: 5.0000e-04\n",
      "Epoch 13/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 626ms/step - accuracy: 0.8259 - loss: 3.8522 - val_accuracy: 0.8994 - val_loss: 3.3106 - learning_rate: 5.0000e-04\n",
      "Epoch 14/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 635ms/step - accuracy: 0.7985 - loss: 3.7007 - val_accuracy: 0.8994 - val_loss: 3.1244 - learning_rate: 5.0000e-04\n",
      "Epoch 15/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 628ms/step - accuracy: 0.8045 - loss: 3.5091 - val_accuracy: 0.9112 - val_loss: 2.9574 - learning_rate: 5.0000e-04\n",
      "Epoch 16/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 629ms/step - accuracy: 0.7864 - loss: 3.3885 - val_accuracy: 0.9172 - val_loss: 2.8149 - learning_rate: 5.0000e-04\n",
      "Epoch 17/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 623ms/step - accuracy: 0.7962 - loss: 3.2517 - val_accuracy: 0.8994 - val_loss: 2.6683 - learning_rate: 5.0000e-04\n",
      "Epoch 18/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 624ms/step - accuracy: 0.7989 - loss: 2.9912 - val_accuracy: 0.8935 - val_loss: 2.5335 - learning_rate: 5.0000e-04\n",
      "Epoch 19/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 623ms/step - accuracy: 0.8103 - loss: 2.8998 - val_accuracy: 0.8994 - val_loss: 2.3968 - learning_rate: 5.0000e-04\n",
      "Epoch 20/20\n",
      "\u001B[1m71/71\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 626ms/step - accuracy: 0.8103 - loss: 2.7416 - val_accuracy: 0.8876 - val_loss: 2.2660 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "despues de entrenamiento balanceado\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 473ms/step - accuracy: 0.8021 - loss: 2.4320\n",
      "Precisión en validación: 88.76%\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 469ms/step - accuracy: 0.8167 - loss: 2.4322\n",
      "Precisión en test: 90.53%\n",
      "\n",
      "Validación\n",
      "Accuracy: 0.8876 (88.76%)\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Alluvial soil     0.8824    0.5769    0.6977        26\n",
      "   Black Soil     0.9655    0.9655    0.9655        58\n",
      "    Clay soil     0.8235    0.8750    0.8485        32\n",
      "     Red soil     0.8500    0.9623    0.9027        53\n",
      "\n",
      "     accuracy                         0.8876       169\n",
      "    macro avg     0.8803    0.8449    0.8536       169\n",
      " weighted avg     0.8896    0.8876    0.8824       169\n",
      "\n",
      "\n",
      "Confianza promedi:\n",
      "  Alluvial soil: 0.8072\n",
      "  Black Soil: 0.9268\n",
      "  Clay soil: 0.8116\n",
      "  Red soil: 0.9216\n",
      "\n",
      "Test\n",
      "Accuracy: 0.9053 (90.53%)\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Alluvial soil     0.8947    0.6296    0.7391        27\n",
      "   Black Soil     0.9815    0.9138    0.9464        58\n",
      "    Clay soil     0.8205    0.9697    0.8889        33\n",
      "     Red soil     0.8947    1.0000    0.9444        51\n",
      "\n",
      "     accuracy                         0.9053       169\n",
      "    macro avg     0.8979    0.8783    0.8797       169\n",
      " weighted avg     0.9100    0.9053    0.9015       169\n",
      "\n",
      "\n",
      "Confianza promedi:\n",
      "  Alluvial soil: 0.7000\n",
      "  Black Soil: 0.8884\n",
      "  Clay soil: 0.8813\n",
      "  Red soil: 0.9275\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T20:39:38.304597Z",
     "start_time": "2025-08-02T20:39:38.290088Z"
    }
   },
   "cell_type": "code",
   "source": "class_names = class_names",
   "id": "4c249855a0a781de",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T20:39:39.471943Z",
     "start_time": "2025-08-02T20:39:39.453944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Class names:\", class_names)\n",
    "print(\"Number of classes:\", len(class_names))"
   ],
   "id": "cfd3df7c94b3fe3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['Alluvial soil', 'Black Soil', 'Clay soil', 'Red soil']\n",
      "Number of classes: 4\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T20:39:41.119962Z",
     "start_time": "2025-08-02T20:39:41.101960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Crear diccionario con los índices de clase\n",
    "class_names = class_names\n",
    "class_indices = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "# Guardar en JSON\n",
    "with open(\"class_indices.json\", \"w\") as f:\n",
    "    json.dump(class_indices, f)\n",
    "\n",
    "print(\"class_indices.json\")"
   ],
   "id": "e2a960030d46dcf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_indices.json\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T20:39:44.935427Z",
     "start_time": "2025-08-02T20:39:44.654313Z"
    }
   },
   "cell_type": "code",
   "source": "model.save(\"soil_classifier.keras\")",
   "id": "e0e348f152d57004",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T20:39:46.705002Z",
     "start_time": "2025-08-02T20:39:46.157091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model = tf.keras.models.load_model(\"soil_classifier.keras\")"
   ],
   "id": "97289fe4b50fccf",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T20:39:52.591995Z",
     "start_time": "2025-08-02T20:39:51.692609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "img_path = \"C:/Users/Natalia/Desktop/PADIA/SIRIA_copernicus/dataset_soil/testing/Clay soil/Clay_13.jpg\"  # Cambia esto por la ruta de tu imagen\n",
    "img_size = (384,384)\n",
    "\n",
    "# Cargar imagen y redimensionar\n",
    "img = load_img(img_path, target_size=img_size)\n",
    "\n",
    "# Convertir a array y escalar\n",
    "img_array = img_to_array(img)\n",
    "img_array = preprocess_input(img_array)  # Normalizar igual que en el modelo\n",
    "\n",
    "# Expandir dimensiones para simular un batch de tamaño 1\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "class_names = class_names\n",
    "\n",
    "predicted_class = np.argmax(predictions[0])\n",
    "confidence = 100 * np.max(predictions[0])\n",
    "\n",
    "print(f\"La imagen probablemente pertenece a '{class_names[predicted_class]}' con una confianza de {confidence:.2f}%\")"
   ],
   "id": "c401eeb064918b1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 845ms/step\n",
      "La imagen probablemente pertenece a 'Clay soil' con una confianza de 96.85%\n"
     ]
    }
   ],
   "execution_count": 65
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
